{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Ray on interactive compute cluster\n",
        "\n",
        "In [ray-on-compute-instance notebook](../1.ray-on-compute-instance/ray-on-compute-instance.ipynb), we learned how to start a local Ray cluster and interactively execute Ray script on compute instance.\n",
        "\n",
        "In [ray-on-compute-cluster](../2.ray-on-compute-cluster/ray-on-compute-cluster.ipynb), we learned how to submit a distributed training job with Ray cluster enabled onto multi-nodes Azure ML compute clusters.\n",
        "\n",
        "In this notebook, we would learn how to combine this 2 scenarios to build a multi-nodes heterogeneous Ray cluster, and interactively execute Ray application.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "To build an interactive multi-nodes heterogeneous Ray cluster, we need one compute instance as head node and one or more cpu/gpu compute clusters as worker nodes.\n",
        "\n",
        "The compute instance and compute cluster are required to be placed in one virtual network and subnet.\n",
        "\n",
        "Please follow [this document](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-secure-training-vnet?view=azureml-api-2&tabs=cli%2Crequired) to setup 1 cpu compute instance and 2 nodes gpu compute cluster."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required packages\n",
        "\n",
        "More info about installing Ray could be found [here](https://docs.ray.io/en/latest/ray-overview/installation.html)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --no-cache-dir \\\n",
        " ../../private_wheel/azure_ai_ml-1.6.0a20230421002-py3-none-any.whl \\\n",
        " 'ray[default, air, tune]==2.4.0' \\\n",
        " gpustat==1.0.0 \\\n",
        " torch \\\n",
        " torchvision"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1682983694785
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start a Ray cluster on compute instance\n",
        "\n",
        "We would use the current compute instance as head node of the Ray cluster we are trying to build."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "import configparser\n",
        "\n",
        "dashboard_port = 8265\n",
        "\n",
        "ray_instance = ray.init(\n",
        "    include_dashboard= True,\n",
        "    dashboard_port=dashboard_port,\n",
        "    ignore_reinit_error=True\n",
        ")\n",
        "\n",
        "\n",
        "# update Ray dashboard link\n",
        "try:\n",
        "    parser = configparser.ConfigParser()\n",
        "    with open(\"/mnt/azmnt/.nbvm\") as stream:\n",
        "        parser.read_string(\"[config]\\n\" + stream.read())\n",
        "\n",
        "    config = parser['config']\n",
        "    ci_name = config['instance']\n",
        "    domainsuffix = config['domainsuffix']\n",
        "\n",
        "    dashboard_url = f'{ci_name}-{dashboard_port}.{domainsuffix}'\n",
        "except:\n",
        "    dashboard_url = ray_instance.dashboard_url\n",
        "\n",
        "ray_instance.dashboard_url = dashboard_url\n",
        "ray_instance"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1682983784336
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attach worker nodes using compute cluster\n",
        "\n",
        "After head node started, we can submit a worker nodes only job by passing the head node address."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import required libraries"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "from azure.ai.ml import MLClient, command\n",
        "from azure.ai.ml.entities import Environment"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1682983758208
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to workspace using DefaultAzureCredential\n",
        "\n",
        "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "credential = DefaultAzureCredential()\n",
        "ml_client = None\n",
        "try:\n",
        "    ml_client = MLClient.from_config(credential)\n",
        "    workspace = ml_client.workspace_name\n",
        "    subscription_id = ml_client.workspaces.get(workspace).id.split(\"/\")[2]\n",
        "    resource_group = ml_client.workspaces.get(workspace).resource_group\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "    # Enter details of your AML workspace\n",
        "    subscription_id = \"<SUBSCRIPTION_ID>\"\n",
        "    resource_group = \"<RESOURCE_GROUP>\"\n",
        "    workspace = \"<AML_WORKSPACE_NAME>\"\n",
        "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1682983761215
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build environment\n",
        "\n",
        "We would use Azure ML image and a conda yaml file to build an environment. More info about how to build environment could be found [here](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-environments-v2?view=azureml-api-2&tabs=python).\n",
        "\n",
        "As Ray requires exact version match of both `python` and `ray`, let's generate a `conda.yml` file matches current kernel.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from platform import python_version\n",
        "\n",
        "# Get and set python and ray version\n",
        "python_version = python_version()\n",
        "ray_version = '2.4.0'\n",
        "\n",
        "conda = yaml.load(f\"\"\"\n",
        "    name: ray-environment\n",
        "    dependencies:\n",
        "    - python={python_version}\n",
        "    - pip:\n",
        "        - ray[default, tune]=={ray_version}\n",
        "        - torch\n",
        "        - torchvision\n",
        "\"\"\", Loader=yaml.CLoader)\n",
        "\n",
        "# Write to conda.yml file\n",
        "with open('conda.yml', 'w') as conda_file:\n",
        "    yaml.dump(conda, conda_file, default_flow_style=False)\n",
        "\n",
        "\n",
        "# Build environment using AzureML image and conda.yml we built\n",
        "environment=Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.3-cudnn8-ubuntu20.04\",\n",
        "    conda_file=\"conda.yml\"\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1682983765206
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure and Run Command\n",
        "In this section we will be configuring and running a `Command` job.\n",
        "\n",
        "The `command` allows user to configure the following key aspects.\n",
        "- `command` - This is the command that needs to be run. In this example, we would execute `sleep infinity` which would block the job to complete.\n",
        "- `environment` - This is the environment needed for the command to run. In this example, we would use the environment we just build.\n",
        "- `compute` - The compute on which the command will run. In this example, we specify the compute we created in the same vnet of current compute instance.\n",
        "- `instance_count` - The number of nodes to use for the job. In this example, we would scale `2` nodes.\n",
        "- `distribution` - Distribution configuration for distributed training scenarios. In this example, we would set it to `ray`. Azure ML job engine would setup Ray cluster automatically.\n",
        "  - `port` - \\[Optional\\] The port of the head ray process. Default is `6379`\n",
        "  - `address` - \\[Optional\\] The address of Ray head node.\n",
        "  - `worker_node_additional_args` - \\[Optional\\] Additional arguments passed to ray start in worker node."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job = command(\n",
        "    experiment_name=\"mnist_pytorch\",\n",
        "    command=\"sleep infinity\",\n",
        "    environment=environment,\n",
        "    compute=\"gpu-cluster\",\n",
        "    instance_count=2,  # In this, only 2 node cluster was created.\n",
        "    distribution={\n",
        "        \"type\": \"ray\",\n",
        "        \"address\": ray_instance.address_info[\"address\"], # [Optional] The address of ray head node\n",
        "        # \"worker_node_additional_args\": \"--verbose\", # [Optional] Additional arguments passed to ray start in head node.\n",
        "    },\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1682983766319
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit the job\n",
        "\n",
        "By submitting the command job, Azure ML would scale up the compute cluster and connect to the head node."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "active_job = ml_client.jobs.create_or_update(job)\n",
        "\n",
        "active_job"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1682983773230
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the training script\n",
        "We would continue to use the same PyTorch example from Ray:\n",
        "[https://github.com/ray-project/ray/blob/master/python/ray/tune/examples/mnist_pytorch.py](https://github.com/ray-project/ray/blob/master/python/ray/tune/examples/mnist_pytorch.py)\n",
        "\n",
        "Script is downloaded into [src/mnist_pytorch.py](./src/mnist_pytorch.py)\n",
        "\n",
        "We would run the application _interactively_ and see the output in real time."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run src/mnist_pytorch.py --cuda"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show Ray cluster resources"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ray.cluster_resources()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1682654040765
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shutdown the head and worker node"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shutdown head node\n",
        "ray.shutdown()\n",
        "\n",
        "# cancel worker job would automaticlaly shutdown worker node\n",
        "poller = ml_client.jobs.begin_cancel(name=active_job.name)\n",
        "\n",
        "# wait until job cancelled\n",
        "poller.wait()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1682654040962
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}